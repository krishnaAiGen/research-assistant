# Research Assistant

AI-powered research assistant for semantic search, document analysis, and paper comparison using OpenAI embeddings and ChromaDB.

## Project Structure

```
research-assistant/
â”œâ”€â”€ backend/                    # FastAPI backend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.py            # API endpoints
â”‚   â”‚   â”œâ”€â”€ models.py          # Data models
â”‚   â”‚   â”œâ”€â”€ embeddings.py      # OpenAI embeddings
â”‚   â”‚   â”œâ”€â”€ vector_store.py    # ChromaDB operations
â”‚   â”‚   â”œâ”€â”€ usage_tracker.py   # Redis analytics
â”‚   â”‚   â””â”€â”€ auth.py            # JWT authentication
â”‚   â”œâ”€â”€ chroma_db/             # Vector database storage
â”‚   â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚   â””â”€â”€ env.example           # Environment variables
â”œâ”€â”€ frontend/                  # Next.js frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/              # App router pages
â”‚   â”‚   â”œâ”€â”€ components/       # React components
â”‚   â”‚   â””â”€â”€ lib/              # Utilities
â”‚   â”œâ”€â”€ package.json          # Node dependencies
â”‚   â””â”€â”€ env.example          # Environment variables
â””â”€â”€ sample_data/              # Test data
```

## Quick Setup

### 1. Prerequisites
```bash
# Install required services
brew install redis          # macOS
# or
sudo apt install redis      # Ubuntu

# Start Redis server
redis-server
```

### 2. Backend Setup
```bash
cd backend

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp env.example .env
# Edit .env with your OpenAI API key

# Start backend
python run.py
```

### 3. Frontend Setup
```bash
cd frontend

# Install dependencies
npm install

# Configure environment
cp env.example .env.local
# Edit .env.local with backend URL

# Start frontend
npm run dev
```

## Environment Variables

### Backend (.env)
```bash
# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=text-embedding-ada-002
OPENAI_CHAT_MODEL=gpt-3.5-turbo

# Database Configuration
CHROMA_DB_PATH=./chroma_db
CHROMA_COLLECTION_NAME=journal_chunks

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=2

# JWT Authentication
JWT_SECRET_KEY=your-super-secret-jwt-key
JWT_EXPIRATION_HOURS=24

# Server Configuration
HOST=0.0.0.0
PORT=8000
DEBUG=True
ALLOWED_ORIGINS=http://localhost:3000
```

### Frontend (.env.local)
```bash
NEXT_PUBLIC_API_URL=http://localhost:8000
OPENAI_API_KEY=your_openai_api_key_here
```

## API Endpoints

| Method | Endpoint | Description | Auth |
|--------|----------|-------------|------|
| GET | `/` | Health check | None |
| POST | `/api/auth/token` | Generate JWT token | None |
| GET | `/api/stats` | Database statistics | None |
| PUT | `/api/upload` | Upload papers | ðŸ”’ Admin |
| POST | `/api/similarity_search` | Search papers | None |
| GET | `/api/{doc_id}` | Get document | None |
| POST | `/api/compare` | Compare papers | None |
| GET | `/api/popular` | Popular papers | ðŸ”’ Analytics |
| GET | `/api/analytics` | Usage analytics | ðŸ”’ Analytics |

## Testing

### Backend API
```bash
cd backend
python test_api.py          # Test all endpoints
python test_auth.py         # Test authentication
```

### Generate Auth Token
```bash
curl -X POST "http://localhost:8000/api/auth/token" \
  -H "Content-Type: application/json" \
  -d '{"user_id": "admin", "role": "admin"}'
```

## Features

- **Semantic Search**: OpenAI embeddings with ChromaDB
- **Paper Comparison**: AI-powered analysis using GPT-3.5-turbo
- **Analytics**: Redis-based usage tracking
- **Authentication**: JWT with role-based access control
- **File Upload**: Local files, URLs, and Google Drive support
- **Real-time**: WebSocket support for live updates

## Architecture

- **Backend**: FastAPI + ChromaDB + Redis + OpenAI
- **Frontend**: Next.js + TypeScript + Tailwind CSS
- **Database**: ChromaDB (vector) + Redis (analytics)
- **Authentication**: JWT tokens with role-based permissions
- **Deployment**: Docker support (optional)

## Usage

1. **Start Services**: Redis â†’ Backend â†’ Frontend
2. **Get Token**: Generate admin token via `/api/auth/token`
3. **Upload Data**: Use token to upload papers via `/api/upload`
4. **Search**: Query papers via `/api/similarity_search`
5. **Analytics**: View usage stats via `/api/analytics`

## Documentation

- **API Documentation**: `backend/API_DOCUMENTATION.md`
- **Frontend README**: `frontend/README.md`

